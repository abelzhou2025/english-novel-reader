<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Can artificial intelligence really help us talk to the animals? - English Novel Reader</title>
    <link rel="stylesheet" href="../styles.css">
    <style>
        /* Article reader specific styles */
        .article-content {
            max-width: 800px;
            margin: 100px auto;
            padding: 30px;
            background-color: #fff;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }
        
        .article-title {
            font-size: 1.8rem;
            color: #5a3e2b;
            margin-bottom: 10px;
            font-family: Georgia, serif;
        }
        
        .article-title-chinese {
            font-size: 1.2rem;
            color: #666;
            margin-bottom: 30px;
            font-family: 'Microsoft YaHei', sans-serif;
            opacity: 0.8;
        }
        
        .article-paragraph {
            margin-bottom: 25px;
            line-height: 1.8;
        }
        
        .english-text {
            font-family: Georgia, serif;
            font-size: 1.1rem;
            color: #333;
            margin-bottom: 15px;
        }
        
        .chinese-text {
            font-family: 'Microsoft YaHei', sans-serif;
            font-size: 1rem;
            color: #666;
            opacity: 0.7;
            margin-bottom: 30px;
            padding-left: 20px;
            border-left: 3px solid #d4a76a;
        }
        
        .back-btn {
            display: inline-block;
            background-color: #5a3e2b;
            color: #fff;
            padding: 10px 20px;
            border-radius: 6px;
            text-decoration: none;
            font-family: 'Microsoft YaHei', sans-serif;
            font-size: 1rem;
            transition: all 0.3s ease;
            margin-bottom: 30px;
        }
        
        .back-btn:hover {
            background-color: #7a5c42;
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }
        
        .loading-translation {
            font-style: italic;
            color: #999;
        }
    </style>
</head>
<body>
    <!-- Top Navigation -->
    <nav class="top-nav">
        <ul>
            <li><a href="../index.html">小说</a></li>
            <li><a href="../webnovels.html">网文</a></li>
            <li><a href="../about.html">关于我</a></li>
        </ul>
    </nav>
    
    <div class="container">
        <div class="main-content">
            <div class="article-content">
                <a href="../webnovels.html" class="back-btn">← 返回网文列表</a>
                <h1 class="article-title">Can artificial intelligence really help us talk to the animals?</h1>
                <div class="article-title-chinese loading-translation" data-english="Can artificial intelligence really help us talk to the animals?">正在翻译标题...</div>
                <div id="articleBody">
                    <div class="article-paragraph">
                        <div class="english-text">A dolphin handler makes the signal for "together" with her hands, followed by "create". The two trained dolphins disappear underwater, exchange sounds and then emerge, flip on to their backs and lift their tails. They have devised a new trick of their own and performed it in tandem, just as requested. "It doesn't prove that there's language," says Aza Raskin. "But it certainly makes a lot of sense that, if they had access to a rich, symbolic way of communicating, that would make this task much easier."</div>
                        <div class="chinese-text loading-translation" data-english="A dolphin handler makes the signal for "together" with her hands, followed by "create". The two trained dolphins disappear underwater, exchange sounds and then emerge, flip on to their backs and lift their tails. They have devised a new trick of their own and performed it in tandem, just as requested. "It doesn't prove that there's language," says Aza Raskin. "But it certainly makes a lot of sense that, if they had access to a rich, symbolic way of communicating, that would make this task much easier."">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">Raskin is the co-founder and president of Earth Species Project (ESP), a California non-profit group with a bold ambition: to decode non-human communication using a form of artificial intelligence (AI) called machine learning, and make all the knowhow publicly available, thereby deepening our connection with other living species and helping to protect them. A 1970 album of whale song) galvanised the movement that led to commercial whaling being banned. What could a Google Translate for the animal kingdom spawn?</div>
                        <div class="chinese-text loading-translation" data-english="Raskin is the co-founder and president of Earth Species Project (ESP), a California non-profit group with a bold ambition: to decode non-human communication using a form of artificial intelligence (AI) called machine learning, and make all the knowhow publicly available, thereby deepening our connection with other living species and helping to protect them. A 1970 album of whale song) galvanised the movement that led to commercial whaling being banned. What could a Google Translate for the animal kingdom spawn?">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">The organisation, founded in 2017 with the help of major donors such as LinkedIn co-founder Reid Hoffman, published its first scientific paper last December. The goal is to unlock communication within our lifetimes. "The end we are working towards is, can we decode animal communication, discover non-human language," says Raskin. "Along the way and equally important is that we are developing technology that supports biologists and conservation now."</div>
                        <div class="chinese-text loading-translation" data-english="The organisation, founded in 2017 with the help of major donors such as LinkedIn co-founder Reid Hoffman, published its first scientific paper last December. The goal is to unlock communication within our lifetimes. "The end we are working towards is, can we decode animal communication, discover non-human language," says Raskin. "Along the way and equally important is that we are developing technology that supports biologists and conservation now."">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">Understanding animal vocalisations has long been the subject of human fascination and study. Various primates give alarm calls that differ according to predator; dolphins address one another with signature whistles; and some songbirds can take elements of their calls and rearrange them to communicate different messages. But most experts stop short of calling it a language, as no animal communication meets all the criteria.</div>
                        <div class="chinese-text loading-translation" data-english="Understanding animal vocalisations has long been the subject of human fascination and study. Various primates give alarm calls that differ according to predator; dolphins address one another with signature whistles; and some songbirds can take elements of their calls and rearrange them to communicate different messages. But most experts stop short of calling it a language, as no animal communication meets all the criteria.">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">Until recently, decoding has mostly relied on painstaking observation. But interest has burgeoned in applying machine learning to deal with the huge amounts of data that can now be collected by modern animal-borne sensors. "People are starting to use it," says Elodie Briefer, an associate professor at the University of Copenhagen who studies vocal communication in mammals and birds. "But we don't really understand yet how much we can do."</div>
                        <div class="chinese-text loading-translation" data-english="Until recently, decoding has mostly relied on painstaking observation. But interest has burgeoned in applying machine learning to deal with the huge amounts of data that can now be collected by modern animal-borne sensors. "People are starting to use it," says Elodie Briefer, an associate professor at the University of Copenhagen who studies vocal communication in mammals and birds. "But we don't really understand yet how much we can do."">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">Briefer co-developed an algorithm that analyses pig grunts to tell whether the animal is experiencing a positive or negative emotion. Another, called DeepSqueak, judges whether rodents are in a stressed state based on their ultrasonic calls. A further initiative -- Project CETI (which stands for the Cetacean Translation Initiative) -- plans to use machine learning to translate the communication of sperm whales.
!</div>
                        <div class="chinese-text loading-translation" data-english="Briefer co-developed an algorithm that analyses pig grunts to tell whether the animal is experiencing a positive or negative emotion. Another, called DeepSqueak, judges whether rodents are in a stressed state based on their ultrasonic calls. A further initiative -- Project CETI (which stands for the Cetacean Translation Initiative) -- plans to use machine learning to translate the communication of sperm whales.
!">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">Earlier this year, Elodie Briefer and colleagues published a study of pigs' emotions based on their vocalisations. 7,414 sounds were collected from 411 pigs in a variety of scenarios. Photograph: Matt Cardy/Getty Images</div>
                        <div class="chinese-text loading-translation" data-english="Earlier this year, Elodie Briefer and colleagues published a study of pigs' emotions based on their vocalisations. 7,414 sounds were collected from 411 pigs in a variety of scenarios. Photograph: Matt Cardy/Getty Images">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">Yet ESP says its approach is different, because it is not focused on decoding the communication of one species, but all of them. While Raskin acknowledges there will be a higher likelihood of rich, symbolic communication among social animals -- for example primates, whales and dolphins -- the goal is to develop tools that could be applied to the entire animal kingdom. "We're species agnostic," says Raskin. "The tools we develop... can work across all of biology, from worms to whales."</div>
                        <div class="chinese-text loading-translation" data-english="Yet ESP says its approach is different, because it is not focused on decoding the communication of one species, but all of them. While Raskin acknowledges there will be a higher likelihood of rich, symbolic communication among social animals -- for example primates, whales and dolphins -- the goal is to develop tools that could be applied to the entire animal kingdom. "We're species agnostic," says Raskin. "The tools we develop... can work across all of biology, from worms to whales."">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">The "motivating intuition" for ESP, says Raskin, is work that has shown that machine learning can be used to translate between different, sometimes distant human languages -- without the need for any prior knowledge.</div>
                        <div class="chinese-text loading-translation" data-english="The "motivating intuition" for ESP, says Raskin, is work that has shown that machine learning can be used to translate between different, sometimes distant human languages -- without the need for any prior knowledge.">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">This process starts with the development of an algorithm to represent words in a physical space. In this many-dimensional geometric representation, the distance and direction between points (words) describes how they meaningfully relate to each other (their semantic relationship). For example, "king" has a relationship to "man" with the same distance and direction that "woman' has to "queen". (The mapping is not done by knowing what the words mean but by looking, for example, at how often they occur near each other.)</div>
                        <div class="chinese-text loading-translation" data-english="This process starts with the development of an algorithm to represent words in a physical space. In this many-dimensional geometric representation, the distance and direction between points (words) describes how they meaningfully relate to each other (their semantic relationship). For example, "king" has a relationship to "man" with the same distance and direction that "woman' has to "queen". (The mapping is not done by knowing what the words mean but by looking, for example, at how often they occur near each other.)">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">It was later noticed that these "shapes" are similar for different languages. And then, in 2017, two groups of researchers working independently found a technique that made it possible to achieve translation by aligning the shapes. To get from English to Urdu, align their shapes and find the point in Urdu closest to the word's point in English. "You can translate most words decently well," says Raskin.</div>
                        <div class="chinese-text loading-translation" data-english="It was later noticed that these "shapes" are similar for different languages. And then, in 2017, two groups of researchers working independently found a technique that made it possible to achieve translation by aligning the shapes. To get from English to Urdu, align their shapes and find the point in Urdu closest to the word's point in English. "You can translate most words decently well," says Raskin.">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">ESP's aspiration is to create these kinds of representations of animal communication -- working on both individual species and many species at once -- and then explore questions such as whether there is overlap with the universal human shape. We don't know how animals experience the world, says Raskin, but there are emotions, for example grief and joy, it seems some share with us and may well communicate about with others in their species. "I don't know which will be the more incredible -- the parts where the shapes overlap and we can directly communicate or translate, or the parts where we can't."
!</div>
                        <div class="chinese-text loading-translation" data-english="ESP's aspiration is to create these kinds of representations of animal communication -- working on both individual species and many species at once -- and then explore questions such as whether there is overlap with the universal human shape. We don't know how animals experience the world, says Raskin, but there are emotions, for example grief and joy, it seems some share with us and may well communicate about with others in their species. "I don't know which will be the more incredible -- the parts where the shapes overlap and we can directly communicate or translate, or the parts where we can't."
!">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">Dolphins use clicks, whistles and other sounds to communicate. But what are they saying? Photograph: ALesik/Getty Images/iStockphoto</div>
                        <div class="chinese-text loading-translation" data-english="Dolphins use clicks, whistles and other sounds to communicate. But what are they saying? Photograph: ALesik/Getty Images/iStockphoto">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">He adds that animals don't only communicate vocally. Bees, for example, let others know of a flower's location via a "waggle dance". There will be a need to translate across different modes of communication too.</div>
                        <div class="chinese-text loading-translation" data-english="He adds that animals don't only communicate vocally. Bees, for example, let others know of a flower's location via a "waggle dance". There will be a need to translate across different modes of communication too.">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">The goal is "like going to the moon", acknowledges Raskin, but the idea also isn't to get there all at once. Rather, ESP's roadmap involves solving a series of smaller problems necessary for the bigger picture to be realised. This should see the development of general tools that can help researchers trying to apply AI to unlock the secrets of species under study.</div>
                        <div class="chinese-text loading-translation" data-english="The goal is "like going to the moon", acknowledges Raskin, but the idea also isn't to get there all at once. Rather, ESP's roadmap involves solving a series of smaller problems necessary for the bigger picture to be realised. This should see the development of general tools that can help researchers trying to apply AI to unlock the secrets of species under study.">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">For example, ESP recently published a paper (and shared its code) on the so called "cocktail party problem" in animal communication, in which it is difficult to discern which individual in a group of the same animals is vocalising in a noisy social environment.</div>
                        <div class="chinese-text loading-translation" data-english="For example, ESP recently published a paper (and shared its code) on the so called "cocktail party problem" in animal communication, in which it is difficult to discern which individual in a group of the same animals is vocalising in a noisy social environment.">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">"To our knowledge, no one has done this end-to-end detangling \[of animal sound\] before," says Raskin. The AI-based model developed by ESP, which was tried on dolphin signature whistles, macaque coo calls and bat vocalisations, worked best when the calls came from individuals that the model had been trained on; but with larger datasets it was able to disentangle mixtures of calls from animals not in the training cohort.</div>
                        <div class="chinese-text loading-translation" data-english=""To our knowledge, no one has done this end-to-end detangling \[of animal sound\] before," says Raskin. The AI-based model developed by ESP, which was tried on dolphin signature whistles, macaque coo calls and bat vocalisations, worked best when the calls came from individuals that the model had been trained on; but with larger datasets it was able to disentangle mixtures of calls from animals not in the training cohort.">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">Another project involves using AI to generate novel animal calls, with humpback whales as a test species. The novel calls -- made by splitting vocalisations into micro-phonemes (distinct units of sound lasting a hundredth of a second) and using a language model to "speak" something whale-like -- can then be played back to the animals to see how they respond. If the AI can identify what makes a random change versus a semantically meaningful one, it brings us closer to meaningful communication, explains Raskin. "It is having the AI speak the language, even though we don't know what it means yet."
!</div>
                        <div class="chinese-text loading-translation" data-english="Another project involves using AI to generate novel animal calls, with humpback whales as a test species. The novel calls -- made by splitting vocalisations into micro-phonemes (distinct units of sound lasting a hundredth of a second) and using a language model to "speak" something whale-like -- can then be played back to the animals to see how they respond. If the AI can identify what makes a random change versus a semantically meaningful one, it brings us closer to meaningful communication, explains Raskin. "It is having the AI speak the language, even though we don't know what it means yet."
!">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">Hawaiian crows are well known for their use of tools but are also believed to have a particularly complex set of vocalisations. Photograph: Minden Pictures/Alamy</div>
                        <div class="chinese-text loading-translation" data-english="Hawaiian crows are well known for their use of tools but are also believed to have a particularly complex set of vocalisations. Photograph: Minden Pictures/Alamy">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">A further project aims to develop an algorithm that ascertains how many call types a species has at its command by applying self-supervised machine learning, which does not require any labelling of data by human experts to learn patterns. In an early test case, it will mine audio recordings made by a team led by Christian Rutz, a professor of biology at the University of St Andrews, to produce an inventory of the vocal repertoire of the Hawaiian crow -- a species that, Rutz discovered, has the ability to make and use tools for foraging and is believed to have a significantly more complex set of vocalisations than other crow species.</div>
                        <div class="chinese-text loading-translation" data-english="A further project aims to develop an algorithm that ascertains how many call types a species has at its command by applying self-supervised machine learning, which does not require any labelling of data by human experts to learn patterns. In an early test case, it will mine audio recordings made by a team led by Christian Rutz, a professor of biology at the University of St Andrews, to produce an inventory of the vocal repertoire of the Hawaiian crow -- a species that, Rutz discovered, has the ability to make and use tools for foraging and is believed to have a significantly more complex set of vocalisations than other crow species.">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">Rutz is particularly excited about the project's conservation value. The Hawaiian crow is critically endangered and only exists in captivity, where it is being bred for reintroduction to the wild. It is hoped that, by taking recordings made at different times, it will be possible to track whether the species's call repertoire is being eroded in captivity -- specific alarm calls may have been lost, for example -- which could have consequences for its reintroduction; that loss might be addressed with intervention. "It could produce a step change in our ability to help these birds come back from the brink," says Rutz, adding that detecting and classifying the calls manually would be labour intensive and error prone.</div>
                        <div class="chinese-text loading-translation" data-english="Rutz is particularly excited about the project's conservation value. The Hawaiian crow is critically endangered and only exists in captivity, where it is being bred for reintroduction to the wild. It is hoped that, by taking recordings made at different times, it will be possible to track whether the species's call repertoire is being eroded in captivity -- specific alarm calls may have been lost, for example -- which could have consequences for its reintroduction; that loss might be addressed with intervention. "It could produce a step change in our ability to help these birds come back from the brink," says Rutz, adding that detecting and classifying the calls manually would be labour intensive and error prone.">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">Meanwhile, another project seeks to understand automatically the functional meanings of vocalisations. It is being pursued with the laboratory of Ari Friedlaender, a professor of ocean sciences at the University of California, Santa Cruz. The lab studies how wild marine mammals, which are difficult to observe directly, behave underwater and runs one of the world's largest tagging programmes. Small electronic "biologging" devices attached to the animals capture their location, type of motion and even what they see (the devices can incorporate video cameras). The lab also has data from strategically placed sound recorders in the ocean.</div>
                        <div class="chinese-text loading-translation" data-english="Meanwhile, another project seeks to understand automatically the functional meanings of vocalisations. It is being pursued with the laboratory of Ari Friedlaender, a professor of ocean sciences at the University of California, Santa Cruz. The lab studies how wild marine mammals, which are difficult to observe directly, behave underwater and runs one of the world's largest tagging programmes. Small electronic "biologging" devices attached to the animals capture their location, type of motion and even what they see (the devices can incorporate video cameras). The lab also has data from strategically placed sound recorders in the ocean.">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">ESP aims to first apply self-supervised machine learning to the tag data to automatically gauge what an animal is doing (for example whether it is feeding, resting, travelling or socialising) and then add the audio data to see whether functional meaning can be given to calls tied to that behaviour. (Playback experiments could then be used to validate any findings, along with calls that have been decoded previously.) This technique will be applied to humpback whale data initially -- the lab has tagged several animals in the same group so it is possible to see how signals are given and received. Friedlaender says he was "hitting the ceiling" in terms of what currently available tools could tease out of the data. "Our hope is that the work ESP can do will provide new insights," he says.</div>
                        <div class="chinese-text loading-translation" data-english="ESP aims to first apply self-supervised machine learning to the tag data to automatically gauge what an animal is doing (for example whether it is feeding, resting, travelling or socialising) and then add the audio data to see whether functional meaning can be given to calls tied to that behaviour. (Playback experiments could then be used to validate any findings, along with calls that have been decoded previously.) This technique will be applied to humpback whale data initially -- the lab has tagged several animals in the same group so it is possible to see how signals are given and received. Friedlaender says he was "hitting the ceiling" in terms of what currently available tools could tease out of the data. "Our hope is that the work ESP can do will provide new insights," he says.">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">But not everyone is as gung ho about the power of AI to achieve such grand aims. Robert Seyfarth is a professor emeritus of psychology at University of Pennsylvania who has studied social behaviour and vocal communication in primates in their natural habitat for more than 40 years. While he believes machine learning can be useful for some problems, such as identifying an animal's vocal repertoire, there are other areas, including the discovery of the meaning and function of vocalisations, where he is sceptical it will add much.</div>
                        <div class="chinese-text loading-translation" data-english="But not everyone is as gung ho about the power of AI to achieve such grand aims. Robert Seyfarth is a professor emeritus of psychology at University of Pennsylvania who has studied social behaviour and vocal communication in primates in their natural habitat for more than 40 years. While he believes machine learning can be useful for some problems, such as identifying an animal's vocal repertoire, there are other areas, including the discovery of the meaning and function of vocalisations, where he is sceptical it will add much.">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">The problem, he explains, is that while many animals can have sophisticated, complex societies, they have a much smaller repertoire of sounds than humans. The result is that the exact same sound can be used to mean different things in different contexts and it is only by studying the context -- who the individual calling is, how are they related to others, where they fall in the hierarchy, who they have interacted with -- that meaning can hope to be established. "I just think these AI methods are insufficient," says Seyfarth. "You've got to go out there and watch the animals."
!</div>
                        <div class="chinese-text loading-translation" data-english="The problem, he explains, is that while many animals can have sophisticated, complex societies, they have a much smaller repertoire of sounds than humans. The result is that the exact same sound can be used to mean different things in different contexts and it is only by studying the context -- who the individual calling is, how are they related to others, where they fall in the hierarchy, who they have interacted with -- that meaning can hope to be established. "I just think these AI methods are insufficient," says Seyfarth. "You've got to go out there and watch the animals."
!">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">A map of animal communication will need to incorporate non-vocal phenomena such as the "waggle dances" of honey bees. Photograph: Ben Birchall/PA</div>
                        <div class="chinese-text loading-translation" data-english="A map of animal communication will need to incorporate non-vocal phenomena such as the "waggle dances" of honey bees. Photograph: Ben Birchall/PA">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">There is also doubt about the concept -- that the shape of animal communication will overlap in a meaningful way with human communication. Applying computer-based analyses to human language, with which we are so intimately familiar, is one thing, says Seyfarth. But it can be "quite different" doing it to other species. "It is an exciting idea, but it is a big stretch," says Kevin Coffey, a neuroscientist at the University of Washington who co-created the DeepSqueak algorithm.</div>
                        <div class="chinese-text loading-translation" data-english="There is also doubt about the concept -- that the shape of animal communication will overlap in a meaningful way with human communication. Applying computer-based analyses to human language, with which we are so intimately familiar, is one thing, says Seyfarth. But it can be "quite different" doing it to other species. "It is an exciting idea, but it is a big stretch," says Kevin Coffey, a neuroscientist at the University of Washington who co-created the DeepSqueak algorithm.">正在翻译...</div>
                    </div>
                    <div class="article-paragraph">
                        <div class="english-text">Raskin acknowledges that AI alone may not be enough to unlock communication with other species. But he refers to research that has shown many species communicate in ways "more complex than humans have ever imagined". The stumbling blocks have been our ability to gather sufficient data and analyse it at scale, and our own limited perception. "These are the tools that let us take off the human glasses and understand entire communication systems," he says.</div>
                        <div class="chinese-text loading-translation" data-english="Raskin acknowledges that AI alone may not be enough to unlock communication with other species. But he refers to research that has shown many species communicate in ways "more complex than humans have ever imagined". The stumbling blocks have been our ability to gather sufficient data and analyse it at scale, and our own limited perception. "These are the tools that let us take off the human glasses and understand entire communication systems," he says.">正在翻译...</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <script src="../script.js"></script>
    <script>
        // Translation cache management
        const TRANSLATION_CACHE = {
            get: (key) => {
                try {
                    const cacheItem = localStorage.getItem('translation_' + key);
                    if (cacheItem) {
                        const parsed = JSON.parse(cacheItem);
                        // Check if cache is still valid (7 days)
                        if (Date.now() - parsed.timestamp < 7 * 24 * 60 * 60 * 1000) {
                            return parsed.translation;
                        }
                    }
                } catch (e) {
                    console.error('Cache read error:', e);
                }
                return null;
            },
            set: (key, translation) => {
                try {
                    localStorage.setItem('translation_' + key, JSON.stringify({
                        translation,
                        timestamp: Date.now()
                    }));
                } catch (e) {
                    console.error('Cache write error:', e);
                }
            }
        };
        
        // Function to translate a paragraph with caching
        async function translateParagraph(text, element) {
            // Generate cache key from text
            const cacheKey = btoa(text.trim().toLowerCase());
            
            // Check if translation is in cache
            const cachedTranslation = TRANSLATION_CACHE.get(cacheKey);
            if (cachedTranslation) {
                element.textContent = cachedTranslation;
                element.classList.remove('loading-translation');
                return;
            }
            
            // Translation APIs configuration
            const apis = [
                {
                    name: 'MyMemory',
                    url: 'https://api.mymemory.translated.net/get?q=' + encodeURIComponent(text) + '&langpair=en|zh'
                },
                {
                    name: 'Google Translate',
                    url: 'https://translate.googleapis.com/translate_a/single?client=gtx&sl=en&tl=zh-CN&dt=t&q=' + encodeURIComponent(text)
                }
            ];
            
            // Try APIs in order
            for (const api of apis) {
                try {
                    const response = await fetch(api.url, {
                        mode: 'cors',
                        headers: {
                            'Content-Type': 'application/json'
                        },
                        // Add timeout for faster failure on slow connections
                        signal: AbortSignal.timeout(5000)
                    });
                    
                    if (api.name === 'MyMemory') {
                        const data = await response.json();
                        if (data.responseStatus === 200 && data.responseData.translatedText) {
                            const translation = data.responseData.translatedText;
                            TRANSLATION_CACHE.set(cacheKey, translation);
                            element.textContent = translation;
                            element.classList.remove('loading-translation');
                            return;
                        }
                    } else if (api.name === 'Google Translate') {
                        const data = await response.json();
                        if (data && data[0] && Array.isArray(data[0])) {
                            const translation = data[0].map(function(item) { return item[0]; }).join('');
                            if (translation) {
                                TRANSLATION_CACHE.set(cacheKey, translation);
                                element.textContent = translation;
                                element.classList.remove('loading-translation');
                                return;
                            }
                        }
                    }
                } catch (error) {
                    console.error(api.name + ' translation error:', error);
                    // Continue to next API if current one fails
                    continue;
                }
            }
            
            // Fallback if all APIs fail
            const fallbackTranslation = element.classList.contains('article-title-chinese') ? '文章标题翻译' : '这是文章内容的中文翻译。';
            element.textContent = fallbackTranslation;
            element.classList.remove('loading-translation');
        }
        
        // Translate all paragraphs when the page loads with optimized concurrency
        document.addEventListener('DOMContentLoaded', function() {
            const chineseTexts = document.querySelectorAll('.chinese-text, .article-title-chinese');
            
            // Limit concurrent translations to avoid overloading API
            const CONCURRENCY_LIMIT = 3;
            let activeTranslations = 0;
            let index = 0;
            
            // Function to process next translation
            const processNext = function() {
                if (index >= chineseTexts.length) return;
                
                // Wait if we've reached concurrency limit
                if (activeTranslations >= CONCURRENCY_LIMIT) {
                    setTimeout(processNext, 100);
                    return;
                }
                
                const element = chineseTexts[index++];
                const englishText = element.getAttribute('data-english');
                
                activeTranslations++;
                translateParagraph(englishText, element)
                    .finally(function() {
                        activeTranslations--;
                        processNext(); // Process next translation when current one finishes
                    });
            };
            
            // Start processing translations
            for (let i = 0; i < CONCURRENCY_LIMIT; i++) {
                processNext();
            }
        });
    </script>
</body>
</html>